{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13591001,"sourceType":"datasetVersion","datasetId":8635247}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================================\n# C√âLULA 1: IMPORTS (COMBINADOS)\n# ============================================================================\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nfrom matplotlib_venn import venn2\n\n# Imports do LOF/ISO\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\nfrom scipy.stats import gaussian_kde\n\n# Imports do LSTM-AE\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    confusion_matrix, classification_report, \n    f1_score, precision_score,\n    recall_score, accuracy_score, jaccard_score,\n    roc_auc_score, precision_recall_curve, auc\n)\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed, Dropout, LayerNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n\nprint(\"=\" * 80)\nprint(\"NOTEBOOK ENSEMBLE: (ISO + LOF) -> LSTM-AUTOENCODER\")\nprint(f\"TensorFlow Vers√£o: {tf.__version__}\")\nprint(\"=\" * 80)\nprint(f\"\\nData/Hora: {pd.Timestamp.now()}\\n\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-11T00:41:13.294096Z","iopub.execute_input":"2025-11-11T00:41:13.294360Z","iopub.status.idle":"2025-11-11T00:41:29.844841Z","shell.execute_reply.started":"2025-11-11T00:41:13.294340Z","shell.execute_reply":"2025-11-11T00:41:29.844052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# C√âLULA 2: LEITURA E PR√â-PROCESSAMENTO\n# ============================================================================\nprint(\"\\n[C√âLULA 2] Leitura e Pr√©-processamento\")\nprint(\"-\" * 80)\n\n# Caminho do dataset\ncaminho = '/kaggle/input/m2-1-dataset/M2.rpt'\nstart = time.time()\n\ndf = pd.read_fwf(caminho)\ncabecalho = df.iloc[1].tolist()\ncabecalho[0] = 'veiculo'\ncabecalho[7] = 'velocidade'\ncabecalho[8] = 'odometro'\n\ndf = df.drop([0, 1, 2]).reset_index(drop=True)\ndf.columns = cabecalho\ndf.columns.name = None\ndf = df.drop(df.index[-1]).reset_index(drop=True)\n\n# Convers√£o de tipos\ncols_numericas = ['Longitude', 'Latitude', 'altitude', 'angle', \n                  'velocidade', 'odometro', 'satellites']\nfor col in cols_numericas:\n    df[col] = df[col].astype(str).str.replace(',', '.').str.strip()\n    df[col] = pd.to_numeric(df[col], errors='coerce')\n\ndf['data'] = pd.to_datetime(df['utcdate'], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\ndf.drop('utcdate', axis=1, inplace=True)\ndf['date'] = df['data'].dt.date\n\nprint(f\"‚úì Dados carregados: {len(df):,} registros\")\nprint(f\"‚úì Tempo: {time.time() - start:.2f}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T00:41:29.846033Z","iopub.execute_input":"2025-11-11T00:41:29.846578Z","iopub.status.idle":"2025-11-11T00:41:47.898288Z","shell.execute_reply.started":"2025-11-11T00:41:29.846557Z","shell.execute_reply":"2025-11-11T00:41:47.897594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# C√âLULA 3: FILTRO DE FREQU√äNCIA\n# ============================================================================\nprint(\"\\n[C√âLULA 3] Filtro de Frequ√™ncia\")\nprint(\"-\" * 80)\n\nfrequencia = df['veiculo'].value_counts()\nveiculos_validos = frequencia[frequencia >= 11].index.tolist()\ndf_antes = len(df)\nveiculos_antes = len(frequencia)\n\ndf = df[df['veiculo'].isin(veiculos_validos)].copy().reset_index(drop=True)\n\nprint(f\"Antes:  {veiculos_antes:,} ve√≠culos, {df_antes:,} registros\")\nprint(f\"Depois: {len(df['veiculo'].unique()):,} ve√≠culos, {len(df):,} registros\")\nprint(f\"Retido: {len(df)/df_antes*100:.1f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T00:41:47.899065Z","iopub.execute_input":"2025-11-11T00:41:47.899340Z","iopub.status.idle":"2025-11-11T00:41:48.455455Z","shell.execute_reply.started":"2025-11-11T00:41:47.899320Z","shell.execute_reply":"2025-11-11T00:41:48.454726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# C√âLULA: EDA TEMPORAL ‚Äî Cobertura Ano √ó M√™s (dias e registros)\n# ============================================================================\nprint(\"\\n[C√âLULA EDA] Cobertura Temporal ‚Äî Anos e Meses com Dados (corrigido)\")\nprint(\"=\" * 90)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 1Ô∏è‚É£ Garantir datetime v√°lido\nif not np.issubdtype(df['data'].dtype, np.datetime64):\n    df['data'] = pd.to_datetime(df['data'], errors='coerce')\n\n# 2Ô∏è‚É£ Colunas auxiliares\ndf_tempo = df[['veiculo', 'data']].copy()\ndf_tempo['ano'] = df_tempo['data'].dt.year\ndf_tempo['mes'] = df_tempo['data'].dt.month\ndf_tempo['dia'] = df_tempo['data'].dt.date\n\n# 3Ô∏è‚É£ Agregar\ncobertura = (\n    df_tempo.groupby(['ano', 'mes'])\n    .agg(\n        qtd_dias=('dia', 'nunique'),\n        qtd_registros=('data', 'count'),\n        qtd_veiculos=('veiculo', 'nunique')\n    )\n    .reset_index()\n)\n\n# 4Ô∏è‚É£ Pivotar para heatmaps\nheatmap_dias = cobertura.pivot(index='ano', columns='mes', values='qtd_dias').fillna(0)\nheatmap_registros = cobertura.pivot(index='ano', columns='mes', values='qtd_registros').fillna(0)\n\n# 5Ô∏è‚É£ Heatmap 1: Dias com registros\nplt.figure(figsize=(6,4))\nsns.heatmap(heatmap_dias, annot=True, fmt='.0f', cmap='YlGnBu', cbar_kws={'label': 'Dias com dados'})\nplt.title('üìÖ Cobertura Temporal ‚Äî Dias com registros por Ano/M√™s', fontsize=14, weight='bold')\nplt.xlabel('M√™s')\nplt.ylabel('Ano')\nplt.tight_layout()\nplt.show()\n\n# 6Ô∏è‚É£ Heatmap 2: Total de registros (densidade real)\nplt.figure(figsize=(6,4))\nsns.heatmap(heatmap_registros, annot=True, fmt='.0f', cmap='YlOrRd', cbar_kws={'label': 'Total de registros'})\nplt.title('‚öôÔ∏è Densidade Temporal ‚Äî Total de registros por Ano/M√™s', fontsize=14, weight='bold')\nplt.xlabel('M√™s')\nplt.ylabel('Ano')\nplt.tight_layout()\nplt.show()\n\n# 7Ô∏è‚É£ Relat√≥rio resumido\nprint(\"\\nüìä Resumo geral de cobertura:\")\nprint(f\"Per√≠odo total: {df_tempo['data'].min()} ‚Üí {df_tempo['data'].max()}\")\nprint(f\"Total de anos: {df_tempo['ano'].nunique()}\")\nprint(f\"Total de meses distintos: {df_tempo[['ano','mes']].drop_duplicates().shape[0]}\")\nprint(f\"M√©dia de dias com dados por m√™s: {cobertura['qtd_dias'].mean():.1f}\")\nprint(f\"M√©dia de registros por m√™s: {cobertura['qtd_registros'].mean():,.0f}\")\nprint(f\"M√©dia de ve√≠culos distintos por m√™s: {cobertura['qtd_veiculos'].mean():.1f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T00:41:48.457510Z","iopub.execute_input":"2025-11-11T00:41:48.457764Z","iopub.status.idle":"2025-11-11T00:41:49.506582Z","shell.execute_reply.started":"2025-11-11T00:41:48.457743Z","shell.execute_reply":"2025-11-11T00:41:49.505878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# C√âLULA 4: ENGENHARIA DE FEATURES COM SEGMENTA√á√ÉO INTELIGENTE\n# ============================================================================\nprint(\"\\n[C√âLULA 4] Engenharia de Features (Movimento, Consist√™ncia e Diagn√≥sticos)\")\nprint(\"-\" * 80)\nstart_features = time.time()\n\n\n# Par√¢metros de segmenta√ß√£o\nGAP_THRESHOLD_S = 600       # 10 minutos\nDISTANCE_JUMP_M = 1000       # salto > 1000km\nSPEED_JUMP_KMH = 30         # salto > 30 km/h\n\n\n\n\n# ----------------------------------------------------------------------------\n# 1) CORRE√á√ÉO DE FORMATO DAS COORDENADAS\n# ----------------------------------------------------------------------------\nprint(\"üîß Corrigindo formato de coordenadas (v√≠rgula ‚Üí ponto)...\")\n\n# Garantir que Latitude e Longitude sejam strings e substituir v√≠rgulas\nfor col in ['Latitude', 'Longitude']:\n    df[col] = (\n        df[col]\n        .astype(str)\n        .str.strip()\n        .str.replace(',', '.', regex=False)\n    )\n    df[col] = pd.to_numeric(df[col], errors='coerce')\n\n# Outras colunas num√©ricas\ncols_numericas = ['altitude', 'angle', 'velocidade', 'odometro', 'satellites']\nfor col in cols_numericas:\n    df[col] = pd.to_numeric(df[col], errors='coerce')\n\n# Ordenar por ve√≠culo e timestamp\ndf_proc = df.sort_values(['veiculo', 'data']).reset_index(drop=True)\n\n# ----------------------------------------------------------------------------\n# 2) VALIDA√á√ÉO DE COORDENADAS GPS (DIN√ÇMICA)\n# ----------------------------------------------------------------------------\nprint(\"  [1/9] Validando coordenadas GPS...\")\n\n# Detectar limites reais (excluindo NaNs)\nlat_min, lat_max = df_proc['Latitude'].min(), df_proc['Latitude'].max()\nlon_min, lon_max = df_proc['Longitude'].min(), df_proc['Longitude'].max()\n\nprint(f\"üåé Limites detectados:\")\nprint(f\"   ‚Ä¢ Latitude:  {lat_min:.6f} ‚Üí {lat_max:.6f}\")\nprint(f\"   ‚Ä¢ Longitude: {lon_min:.6f} ‚Üí {lon_max:.6f}\")\n\n# Definir margens de seguran√ßa (5%) para evitar cortar dados v√°lidos por ru√≠do\nlat_margin = (lat_max - lat_min) * 0.05\nlon_margin = (lon_max - lon_min) * 0.05\n\nlat_min_adj, lat_max_adj = lat_min - lat_margin, lat_max + lat_margin\nlon_min_adj, lon_max_adj = lon_min - lon_margin, lon_max + lon_margin\n\n# Aplicar valida√ß√£o din√¢mica\ndf_proc['coords_validas'] = (\n    df_proc['Latitude'].between(lat_min_adj, lat_max_adj) &\n    df_proc['Longitude'].between(lon_min_adj, lon_max_adj)\n)\n\ncoords_invalidas = (~df_proc['coords_validas']).sum()\nprint(f\"‚ö†Ô∏è  Coordenadas inv√°lidas: {coords_invalidas:,} ({coords_invalidas/len(df_proc)*100:.2f}%)\")\n\n# Substituir coordenadas inv√°lidas por NaN\ndf_proc.loc[~df_proc['coords_validas'], ['Latitude', 'Longitude']] = np.nan\n\n# ----------------------------------------------------------------------------\n# 3) FUN√á√ÉO AUXILIAR: DIST√ÇNCIA HAVERSINE\n# ----------------------------------------------------------------------------\ndef haversine_distance_series(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Calcula a dist√¢ncia Haversine entre pontos consecutivos (em metros).\n    Retorna Series com √≠ndice preservado.\n    \"\"\"\n    lat1_rad, lon1_rad = np.radians(lat1), np.radians(lon1)\n    lat2_rad, lon2_rad = np.radians(lat2), np.radians(lon2)\n    \n    dlat = lat2_rad - lat1_rad\n    dlon = lon2_rad - lon1_rad\n    \n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    distance_m = 6371000.0 * c  # Raio da Terra em metros\n    \n    # Criar Series pandas com √≠ndice original\n    result = pd.Series(distance_m, index=lat1.index)\n    result = result.where(result <= 500000, np.nan)  # descartar saltos > 500 km\n    return result\n\n# ----------------------------------------------------------------------------\n# 4) C√ÅLCULO DE DIFEREN√áAS TEMPORAIS E ESPACIAIS\n# ----------------------------------------------------------------------------\nprint(\"  [2/9] Calculando diferen√ßas temporais e espaciais...\")\n\n# Diferen√ßa temporal (em segundos)\ndf_proc['dt_s'] = df_proc.groupby('veiculo')['data'].diff().dt.total_seconds().fillna(0)\n\n# Diferen√ßa espacial (em metros) usando Haversine\ndf_proc['dx_m'] = haversine_distance_series(\n    df_proc['Latitude'].shift(1), \n    df_proc['Longitude'].shift(1),\n    df_proc['Latitude'], \n    df_proc['Longitude']\n).fillna(0)\n\n# Diferen√ßa de velocidade (km/h)\ndf_proc['dv_kmh'] = df_proc.groupby('veiculo')['velocidade'].diff().abs().fillna(0)\n\n# ----------------------------------------------------------------------------\n# 5) DETEC√á√ÉO DE RUPTURAS E SEGMENTA√á√ÉO INTELIGENTE\n# ----------------------------------------------------------------------------\nprint(\"  [3/9] Detectando rupturas e criando segmentos...\")\n\n# Criar coluna de data (apenas dia)\ndf_proc['data_date'] = pd.to_datetime(df_proc['data']).dt.date\n\n# FLAGS DE RUPTURA\n# 1. Mudan√ßa de ve√≠culo\ndf_proc['ruptura_id'] = (df_proc['veiculo'] != df_proc['veiculo'].shift(1)).astype(int)\n\n# 2. Mudan√ßa de dia\ndf_proc['ruptura_dia'] = (df_proc['data_date'] != df_proc['data_date'].shift(1)).astype(int)\n\n# 3. Gap temporal grande\ndf_proc['ruptura_gap_tempo'] = (df_proc['dt_s'] > GAP_THRESHOLD_S).astype(int)\n\n# 4. Salto espacial anormal\ndf_proc['ruptura_gap_dist'] = (df_proc['dx_m'] > DISTANCE_JUMP_M).astype(int)\n\n# 5. Salto de velocidade anormal\ndf_proc['ruptura_vel'] = (df_proc['dv_kmh'] > SPEED_JUMP_KMH).astype(int)\n\n# Flag agregada de ruptura\ndf_proc['ruptura_any'] = df_proc[[\n    'ruptura_id', 'ruptura_dia', 'ruptura_gap_tempo', \n    'ruptura_gap_dist', 'ruptura_vel'\n]].any(axis=1).astype(int)\n\n# Criar ID de segmento\ndf_proc['segment_id'] = df_proc.groupby('veiculo')['ruptura_any'].cumsum()\n\n# Feature: dist√¢ncia desde √∫ltima ruptura\ndf_proc['since_last_rupture'] = (\n    df_proc.groupby(['veiculo', 'segment_id']).cumcount() + 1\n)\n\n# ----------------------------------------------------------------------------\n# 6) FEATURES DE MOVIMENTO (RESPEITANDO SEGMENTOS)\n# ----------------------------------------------------------------------------\nprint(\"  [4/9] Calculando features de movimento...\")\n\n# Dist√¢ncia acumulada no segmento (em km)\ndf_proc['distancia_km'] = df_proc['dx_m'] / 1000.0\n\n# Velocidade calculada (km/h)\neps = 1e-9\ndf_proc['speed_calc_kmh'] = np.where(\n    df_proc['dt_s'] > 0,\n    (df_proc['dx_m'] / (df_proc['dt_s'] + eps)) * 3.6,\n    0.0\n)\ndf_proc['speed_calc_kmh'] = np.where(\n    df_proc['speed_calc_kmh'] > 300, \n    np.nan, \n    df_proc['speed_calc_kmh']\n)\n\n# Diferen√ßa entre velocidade reportada e calculada\ndf_proc['speed_diff_kmh'] = (\n    df_proc['velocidade'].fillna(0) - df_proc['speed_calc_kmh'].fillna(0)\n)\n\n# ----------------------------------------------------------------------------\n# 7) ACELERA√á√ÉO E MUDAN√áA DE √ÇNGULO\n# ----------------------------------------------------------------------------\nprint(\"  [5/9] Calculando acelera√ß√£o e mudan√ßa de √¢ngulo...\")\n\n# Acelera√ß√£o (m/s¬≤)\ndf_proc['delta_velocidade'] = df_proc.groupby(['veiculo', 'segment_id'])['velocidade'].diff().fillna(0)\ndf_proc['aceleracao'] = np.where(\n    df_proc['dt_s'] > 0,\n    ((df_proc['delta_velocidade'] / 3.6) / (df_proc['dt_s'] + eps)),\n    0.0\n).clip(-10, 10)\n\n# Mudan√ßa de √¢ngulo (considerando wrap-around em 360¬∞)\ndf_proc['mudanca_angulo'] = (\n    df_proc.groupby(['veiculo', 'segment_id'])['angle']\n    .diff()\n    .fillna(0)\n    .abs()\n)\ndf_proc['mudanca_angulo'] = df_proc['mudanca_angulo'].apply(\n    lambda x: x if x <= 180 else 360 - x\n)\n\n# Varia√ß√£o de sat√©lites\ndf_proc['delta_satellites'] = (\n    df_proc.groupby(['veiculo', 'segment_id'])['satellites']\n    .diff()\n    .fillna(0)\n    .abs()\n)\n\n# ----------------------------------------------------------------------------\n# 8) GROUND TRUTH HEUR√çSTICO (DIAGN√ìSTICO)\n# ----------------------------------------------------------------------------\nprint(\"  [6/9] Criando ground truth heur√≠stico...\")\n\ndf_proc['anomalia_confirmada_original'] = 0\n\n# Anomalias: velocidade incompat√≠vel com status do motor\ndf_proc.loc[\n    (df_proc['engineStatus'] == 'parked') & (df_proc['velocidade'] > 2), \n    'anomalia_confirmada_original'\n] = 1\n\ndf_proc.loc[\n    (df_proc['engineStatus'] == 'idling') & (df_proc['velocidade'] > 5), \n    'anomalia_confirmada_original'\n] = 1\n\ndf_proc.loc[\n    (df_proc['engineStatus'] == 'motion') & (df_proc['velocidade'] < 1), \n    'anomalia_confirmada_original'\n] = 1\n\n# ----------------------------------------------------------------------------\n# 9) ENCODING CATEG√ìRICO\n# ----------------------------------------------------------------------------\nprint(\"  [7/9] Aplicando encoding categ√≥rico...\")\ndf_encoded = pd.get_dummies(\n    df_proc, \n    columns=['engineStatus'], \n    prefix='status', \n    drop_first=True\n)\n\n# ----------------------------------------------------------------------------\n# 10) DIAGN√ìSTICO FINAL\n# ----------------------------------------------------------------------------\nprint(\"\\nüìä Diagn√≥stico Final:\")\nprint(f\"‚úì Features criadas em {time.time() - start_features:.2f}s\")\nprint(f\"  ‚Ä¢ Registros totais: {len(df_encoded):,}\")\nprint(f\"  ‚Ä¢ Segmentos √∫nicos: {df_encoded['segment_id'].nunique():,}\")\n\n# Estat√≠sticas por segmento\nstats_segmento = df_encoded.groupby(['veiculo', 'segment_id']).agg({\n    'distancia_km': 'sum',\n    'dt_s': 'sum',\n    'satellites': 'mean',\n    'data': 'count'\n}).rename(columns={'data': 'n_registros', 'dt_s': 'duracao_s'})\n\nprint(f\"\\nüöó Estat√≠sticas de Segmentos:\")\nprint(f\"  ‚Ä¢ Dist√¢ncia m√©dia por segmento: {stats_segmento['distancia_km'].mean():.2f} km\")\nprint(f\"  ‚Ä¢ Dura√ß√£o m√©dia por segmento: {stats_segmento['duracao_s'].mean()/60:.2f} min\")\nprint(f\"  ‚Ä¢ Segmentos com apenas 1 registro: {(stats_segmento['n_registros'] == 1).sum():,}\")\n\n# Detec√ß√£o de anomalias espaciais\nsegmentos_suspeitos = stats_segmento[stats_segmento['distancia_km'] > 100]\nif len(segmentos_suspeitos) > 0:\n    print(f\"‚ö†Ô∏è  Segmentos com dist√¢ncias >100 km: {len(segmentos_suspeitos):,}\")\nelse:\n    print(\"‚úÖ Nenhuma dist√¢ncia anormal entre segmentos\")\n\nprint(\"\\nüì° Qualidade GPS:\")\nprint(f\"  ‚Ä¢ Registros com <4 sat√©lites: {(df_encoded['satellites'] < 4).sum():,} \"\n      f\"({(df_encoded['satellites'] < 4).mean()*100:.1f}%)\")\nprint(f\"  ‚Ä¢ M√©dia global de sat√©lites: {df_encoded['satellites'].mean():.2f}\")\n\nprint(\"\\nüéØ Rupturas Detectadas:\")\nprint(f\"  ‚Ä¢ Mudan√ßas de ve√≠culo: {df_encoded['ruptura_id'].sum():,}\")\nprint(f\"  ‚Ä¢ Mudan√ßas de dia: {df_encoded['ruptura_dia'].sum():,}\")\nprint(f\"  ‚Ä¢ Gaps temporais (>{GAP_THRESHOLD_S}s): {df_encoded['ruptura_gap_tempo'].sum():,}\")\nprint(f\"  ‚Ä¢ Saltos espaciais (>{DISTANCE_JUMP_M}m): {df_encoded['ruptura_gap_dist'].sum():,}\")\nprint(f\"  ‚Ä¢ Saltos de velocidade (>{SPEED_JUMP_KMH} km/h): {df_encoded['ruptura_vel'].sum():,}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ Engenharia de Features Conclu√≠da!\")\nprint(\"=\"*80)","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-11-12T16:59:39.617216Z","iopub.execute_input":"2025-11-12T16:59:39.617404Z","iopub.status.idle":"2025-11-12T16:59:39.709340Z","shell.execute_reply.started":"2025-11-12T16:59:39.617388Z","shell.execute_reply":"2025-11-12T16:59:39.708386Z"}},"outputs":[{"name":"stdout","text":"\n[C√âLULA 4] Engenharia de Features (Movimento, Consist√™ncia e Diagn√≥sticos)\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_39/1660750338.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n[C√âLULA 4] Engenharia de Features (Movimento, Consist√™ncia e Diagn√≥sticos)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstart_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# ----------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"],"ename":"NameError","evalue":"name 'time' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# C√âLULA 5: ENGENHARIA DE FEATURES (TEMPORAIS AVAN√áADAS)\n# ============================================================================\nprint(\"\\n[C√âLULA 5] Engenharia de Features (Temporais Avan√ßadas)\")\nprint(\"-\" * 80)\nstart_time_temporal = time.time()\n\n# 1. Extrair componentes num√©ricos b√°sicos\nprint(\"  [1/3] Extraindo componentes b√°sicos...\")\ndf_encoded['hora'] = df_encoded['data'].dt.hour\ndf_encoded['dia_semana'] = df_encoded['data'].dt.dayofweek # 0=Segunda, 6=Domingo\ndf_encoded['mes'] = df_encoded['data'].dt.month\n\n# 2. Criar Features C√≠clicas (Seno/Cosseno)\nprint(\"  [2/3] Criando features c√≠clicas (sin/cos)...\")\ndf_encoded['hora_sin'] = np.sin(2 * np.pi * df_encoded['hora'] / 24.0)\ndf_encoded['hora_cos'] = np.cos(2 * np.pi * df_encoded['hora'] / 24.0)\ndf_encoded['dia_semana_sin'] = np.sin(2 * np.pi * df_encoded['dia_semana'] / 7.0)\ndf_encoded['dia_semana_cos'] = np.cos(2 * np.pi * df_encoded['dia_semana'] / 7.0)\ndf_encoded['mes_sin'] = np.sin(2 * np.pi * df_encoded['mes'] / 12.0)\ndf_encoded['mes_cos'] = np.cos(2 * np.pi * df_encoded['mes'] / 12.0)\n\n# 3. Criar Features Contextuais (Bandeiras Bin√°rias)\nprint(\"  [3/3] Criando features contextuais (bin√°rias)...\")\ndf_encoded['is_fim_de_semana'] = df_encoded['dia_semana'].isin([5, 6]).astype(int)\ndf_encoded['is_hora_rush'] = ((df_encoded['hora'] >= 7) & (df_encoded['hora'] <= 9) |\n                             (df_encoded['hora'] >= 17) & (df_encoded['hora'] <= 19)).astype(int)\ndf_encoded['is_madrugada'] = (df_encoded['hora'] <= 4).astype(int)\n\nprint(f\"\\n‚úì Novas features temporais criadas em {time.time() - start_time_temporal:.2f}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T00:48:33.437457Z","iopub.execute_input":"2025-11-11T00:48:33.437766Z","iopub.status.idle":"2025-11-11T00:48:33.566400Z","shell.execute_reply.started":"2025-11-11T00:48:33.437748Z","shell.execute_reply":"2025-11-11T00:48:33.565515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# C√âLULA 6: ENSEMBLE HIER√ÅRQUICO ISO ‚Üí LOF\n# ============================================================================\nprint(\"\\n[C√âLULA 6] Ensemble Hier√°rquico ISO ‚Üí LOF (Otimizado)\")\nprint(\"=\" * 90)\n\n# ----------------------------------------------------------------------------\n# ETAPA 1: PREPARA√á√ÉO DE FEATURES\n# ----------------------------------------------------------------------------\nFEATURES_BASE = [\n    'Longitude', 'Latitude', 'altitude', 'angle',\n    'velocidade', 'odometro', 'satellites',\n    'distancia_km', 'delta_tempo_s', 'aceleracao',\n    'mudanca_angulo', 'delta_satellites'\n]\ndf_modelo = df_encoded[FEATURES_BASE].copy()\n\n# Converter para num√©rico\nfor col in df_modelo.columns:\n    df_modelo[col] = pd.to_numeric(df_modelo[col], errors='coerce')\n\n# Limpar dados\ndf_modelo = df_modelo.replace([np.inf, -np.inf], np.nan)\ndf_modelo = df_modelo.fillna(df_modelo.mean(numeric_only=True))\ndf_modelo = df_modelo.dropna()\n\n# Normalizar\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df_modelo.values)\nprint(f\"‚úì Dataset: {X_scaled.shape[0]:,} amostras, {X_scaled.shape[1]} features\")\n\n# ----------------------------------------------------------------------------\n# ETAPA 2: ISOLATION FOREST (R√ÅPIDO)\n# ----------------------------------------------------------------------------\nprint(\"\\nüå≤ Isolation Forest...\")\nstart = time.time()\n\niso_model = IsolationForest(\n    n_estimators=200,\n    contamination='auto',  # ‚Üê Melhor que valor fixo\n    random_state=42,\n    n_jobs=-1,\n    max_samples=min(100000, len(X_scaled))  # ‚Üê LIMITADOR IMPORTANTE\n)\n\niso_pred = iso_model.fit_predict(X_scaled)\niso_score = iso_model.score_samples(X_scaled)\ntempo_iso = time.time() - start\n\ndf_encoded['iso_prediction'] = iso_pred\ndf_encoded['iso_score'] = iso_score\n\ninliers_mask = iso_pred == 1\nanomalias_iso = (~inliers_mask).sum()\npct_iso = (anomalias_iso / len(df_encoded)) * 100\n\nprint(f\"‚úì Tempo ISO: {tempo_iso:.2f}s\")\nprint(f\"‚úì Inliers: {inliers_mask.sum():,} ({inliers_mask.mean()*100:.1f}%)\")\nprint(f\"‚úì Anomalias: {anomalias_iso:,} ({pct_iso:.2f}%)\")\n\n# ----------------------------------------------------------------------------\n# ETAPA 3: LOF\n# ----------------------------------------------------------------------------\nprint(\"\\nüîé LOF (com amostragem estratificada)...\")\n\n# ‚ö° OTIMIZA√á√ÉO 1: Limitar tamanho do treinamento\nMAX_SAMPLES_LOF = 70000  # ‚Üê CHAVE DA OTIMIZA√á√ÉO!\nnp.random.seed(42)\n\nX_inliers = X_scaled[inliers_mask]\n\nif len(X_inliers) > MAX_SAMPLES_LOF:\n    print(f\"‚ö° Amostrando {MAX_SAMPLES_LOF:,} de {len(X_inliers):,} inliers para treino LOF\")\n    indices = np.random.choice(len(X_inliers), MAX_SAMPLES_LOF, replace=False)\n    X_lof_train = X_inliers[indices]\nelse:\n    X_lof_train = X_inliers\n\n# ‚ö° OTIMIZA√á√ÉO 2: Reduzir n_neighbors\nn_neighbors = min(20, len(X_lof_train) // 100)  # M√°ximo 20 vizinhos\n\nprint(f\"‚úì Treinando LOF com {len(X_lof_train):,} amostras e {n_neighbors} vizinhos\")\n\nstart = time.time()\nlof_model = LocalOutlierFactor(\n    n_neighbors=n_neighbors,\n    contamination='auto',\n    novelty=True,\n    n_jobs=-1,\n    algorithm='auto'  # Deixa sklearn escolher o melhor\n)\n\nlof_model.fit(X_lof_train)\n\n# ‚ö° OTIMIZA√á√ÉO 3: Processar em batches\nBATCH_SIZE = 10000\nlof_scores = []\n\nprint(f\"‚úì Calculando scores em batches de {BATCH_SIZE:,}...\")\nfor i in range(0, len(X_scaled), BATCH_SIZE):\n    batch = X_scaled[i:i+BATCH_SIZE]\n    scores = lof_model.score_samples(batch)\n    lof_scores.extend(scores)\n    if (i // BATCH_SIZE) % 10 == 0:\n        print(f\"  Processado: {i+len(batch):,}/{len(X_scaled):,}\")\n\nlof_score = np.array(lof_scores)\nlof_pred = lof_model.predict(X_scaled)\ntempo_lof = time.time() - start\n\ndf_encoded['lof_score'] = lof_score\ndf_encoded['lof_prediction'] = lof_pred\n\nanomalias_lof = (lof_pred == -1).sum()\npct_lof = (anomalias_lof / len(df_encoded)) * 100\n\nprint(f\"‚úì LOF conclu√≠do em {tempo_lof:.2f}s ({tempo_lof/60:.1f}min)\")\nprint(f\"‚úì Anomalias: {anomalias_lof:,} ({pct_lof:.2f}%)\")\n\n# ----------------------------------------------------------------------------\n# ETAPA 4: NORMALIZA√á√ÉO DOS SCORES\n# ----------------------------------------------------------------------------\nprint(\"\\nüìà Normalizando scores...\")\n\ndf_encoded['iso_score_adj'] = -df_encoded['iso_score'].astype(float)\ndf_encoded['lof_score_adj'] = -df_encoded['lof_score'].astype(float)\n\nscaler_score = MinMaxScaler()\nscore_df = pd.DataFrame({\n    'iso_adj': df_encoded['iso_score_adj'].fillna(df_encoded['iso_score_adj'].median()),\n    'lof_adj': df_encoded['lof_score_adj'].fillna(df_encoded['lof_score_adj'].median())\n})\nscore_scaled = scaler_score.fit_transform(score_df)\ndf_encoded['iso_score_norm'] = score_scaled[:, 0]\ndf_encoded['lof_score_norm'] = score_scaled[:, 1]\n\n# ----------------------------------------------------------------------------\n# ETAPA 5: RELAT√ìRIO\n# ----------------------------------------------------------------------------\nprint(\"\\nüìä RESUMO ISO ‚Üí LOF\")\nprint(\"-\" * 80)\nprint(f\"ISO detectou {anomalias_iso:,} anomalias ({pct_iso:.2f}%)\")\nprint(f\"LOF detectou {anomalias_lof:,} anomalias ({pct_lof:.2f}%)\")\nprint(f\"Tempo: ISO {tempo_iso:.1f}s | LOF {tempo_lof:.1f}s\")\nprint(f\"Speedup ISO: {tempo_lof/tempo_iso:.1f}x mais r√°pido\")\n\nprint(\"\\n‚úÖ Modelos prontos para Ground Truth (C√©lula 9).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T00:48:33.567331Z","iopub.execute_input":"2025-11-11T00:48:33.567623Z","iopub.status.idle":"2025-11-11T01:12:52.562596Z","shell.execute_reply.started":"2025-11-11T00:48:33.567598Z","shell.execute_reply":"2025-11-11T01:12:52.561791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# C√âLULA 7: AN√ÅLISE DE CORRELA√á√ÉO ENTRE FEATURES USADAS (ISO ‚Üí LOF)\n# ============================================================================\nprint(\"\\n[C√âLULA 7] An√°lise de Correla√ß√£o - Features Telemetria\")\nprint(\"=\" * 80)\n\nmatriz_corr = df_modelo.corr()\n\nfig, ax = plt.subplots(figsize=(16, 14)) \nmask = np.triu(np.ones_like(matriz_corr, dtype=bool))\nsns.heatmap(matriz_corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n            center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": .7},\n            vmin=-1, vmax=1, ax=ax, annot_kws={'size': 7})\nax.set_title('Matriz de Correla√ß√£o - Features Usadas (ISO ‚Üí LOF)', fontsize=14, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:12:52.563353Z","iopub.execute_input":"2025-11-11T01:12:52.563596Z","iopub.status.idle":"2025-11-11T01:12:54.923813Z","shell.execute_reply.started":"2025-11-11T01:12:52.563575Z","shell.execute_reply":"2025-11-11T01:12:54.922997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# C√âLULA 8: BENCHMARK DE TEMPO (LOF/ISO) (Revisada)\n# ============================================================================\nprint(\"\\n[C√âLULA 8] Benchmark de Performance (LOF/ISO)\")\nprint(\"=\" * 80)\n\nbenchmark_df = pd.DataFrame({\n    'Modelo': ['LOF', 'Isolation Forest'],\n    'Tempo (min)': [tempo_lof/60, tempo_iso/60],\n    'Velocidade (reg/s)': [len(df_encoded)/tempo_lof, len(df_encoded)/tempo_iso],\n    'Anomalias Detectadas': [anomalias_lof, anomalias_iso],\n    'Taxa (%)': [pct_lof, pct_iso]\n})\n\nprint(f\"\\n{benchmark_df.to_string(index=False)}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\ncores = ['#FF6B6B', '#4ECDC4']\nmodelos = ['LOF', 'ISO']\n\n# Gr√°fico 1: Tempo de Execu√ß√£o\ntempos = [tempo_lof, tempo_iso]\naxes[0].barh(modelos, tempos, color=cores, alpha=0.8, edgecolor='black', linewidth=1.5)\naxes[0].set_xlabel('Tempo de Execu√ß√£o (segundos)', fontweight='bold', fontsize=11)\naxes[0].set_title('Compara√ß√£o de Tempo de Execu√ß√£o', fontweight='bold', fontsize=12, pad=15)\naxes[0].grid(axis='x', alpha=0.3)\nfor i, v in enumerate(tempos):\n    axes[0].text(v + max(tempos)*0.02, i, f'{v:.1f}s', va='center', fontweight='bold')\n\n# Gr√°fico 2: Velocidade de Processamento\nvelocidades = [len(df_encoded)/tempo_lof, len(df_encoded)/tempo_iso]\naxes[1].barh(modelos, velocidades, color=cores, alpha=0.8, edgecolor='black', linewidth=1.5)\naxes[1].set_xlabel('Velocidade (registros/segundo)', fontweight='bold', fontsize=11)\naxes[1].set_title('Compara√ß√£o de Velocidade de Processamento', fontweight='bold', fontsize=12, pad=15)\naxes[1].grid(axis='x', alpha=0.3)\nfor i, v in enumerate(velocidades):\n    axes[1].text(v + max(velocidades)*0.02, i, f'{v:,.0f}', va='center', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\nprint(\"\\n‚úì Gr√°ficos de benchmark exibidos\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:12:54.924635Z","iopub.execute_input":"2025-11-11T01:12:54.924845Z","iopub.status.idle":"2025-11-11T01:12:55.157804Z","shell.execute_reply.started":"2025-11-11T01:12:54.924829Z","shell.execute_reply":"2025-11-11T01:12:55.156902Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**PARTE 2: CRIA√á√ÉO DO GROUND TRUTH DE CONSENSO**","metadata":{}},{"cell_type":"code","source":"# === C√âLULA 9: CRIA√á√ÉO DO GROUND TRUTH DE CONSENSO (implementa√ß√£o completa) ===\nprint(\"\\n[C√âLULA 9] Cria√ß√£o do Ground Truth de Consenso (LOF + ISO) - implementa√ß√£o\")\n\n# Par√¢metros de fus√£o\nPERCENTIL_CORTE = 95            # percentil para considerar \"an√¥malo\" quando usando percentil\nTHRESHOLD_ABSOLUTO = None       # ex: 0.85 (usar None se preferir percentil adaptativo)\nPESO_LOF = 0.5\nPESO_ISO = 0.5\n\n# normaliza√ß√£o robusta dos scores se ainda n√£o estiverem normalizados\ndef minmax_series(s):\n    return (s - s.min()) / (s.max() - s.min() + 1e-9)\n\nif 'score_lof' in df_encoded.columns:\n    df_encoded['score_lof_n'] = minmax_series(df_encoded['score_lof'])\nif 'score_iso' in df_encoded.columns:\n    df_encoded['score_iso_n'] = minmax_series(df_encoded['score_iso'])\n\n# estrat√©gia 1: voto simples (predictions j√° {-1,1} ou {0,1})\n# padronizar para 0/1:\ndef to_binary(pred):\n    return ((pred == -1) | (pred == 1) & (pred == -1)).astype(int) if pred.dtype == 'int' else (pred == -1).astype(int)\n\n# tentar inferir colunas de prediction\nif 'pred_lof' in df_encoded.columns:\n    df_encoded['pred_lof_bin'] = (df_encoded['pred_lof'] == -1).astype(int)\nif 'pred_iso' in df_encoded.columns:\n    df_encoded['pred_iso_bin'] = (df_encoded['pred_iso'] == -1).astype(int)\n\n# estrat√©gia de voto ponderado por score normalizado (mais robusta)\ndf_encoded['score_ensemble'] = 0.0\ncomponents = []\nif 'score_lof_n' in df_encoded.columns:\n    df_encoded['score_ensemble'] += PESO_LOF * df_encoded['score_lof_n']\n    components.append('LOF')\nif 'score_iso_n' in df_encoded.columns:\n    df_encoded['score_ensemble'] += PESO_ISO * df_encoded['score_iso_n']\n    components.append('ISO')\n\n# normalizar score_ensemble\ndf_encoded['score_ensemble_n'] = minmax_series(df_encoded['score_ensemble'])\n\n# decis√£o: usar THRESHOLD_ABSOLUTO se fornecido, sen√£o usar percentil\nif THRESHOLD_ABSOLUTO is not None:\n    df_encoded['anomalia_consenso'] = (df_encoded['score_ensemble_n'] >= THRESHOLD_ABSOLUTO).astype(int)\nelse:\n    cutoff = np.percentile(df_encoded['score_ensemble_n'].dropna(), PERCENTIL_CORTE)\n    df_encoded['anomalia_consenso'] = (df_encoded['score_ensemble_n'] >= cutoff).astype(int)\n\n# confian√ßa / explicabilidade: quantas detectaram anomalia?\nif 'pred_lof_bin' in df_encoded.columns and 'pred_iso_bin' in df_encoded.columns:\n    df_encoded['votes_count'] = df_encoded[['pred_lof_bin','pred_iso_bin']].sum(axis=1)\nelse:\n    df_encoded['votes_count'] = np.nan\n\n# Debug / relat√≥rio\ntotal_registros = len(df_encoded)\nanomalias_lof = df_encoded['pred_lof_bin'].sum() if 'pred_lof_bin' in df_encoded.columns else 0\nanomalias_iso = df_encoded['pred_iso_bin'].sum() if 'pred_iso_bin' in df_encoded.columns else 0\ntotal_consenso = df_encoded['anomalia_consenso'].sum()\npct_consenso = total_consenso / total_registros * 100\n\nprint(f\"\\nEstrat√©gia: voto ponderado por score ({', '.join(components)})\")\nprint(f\"PERCENTIL_CORTE={PERCENTIL_CORTE}, THRESHOLD_ABSOLUTO={THRESHOLD_ABSOLUTO}\")\nprint(f\"Anomalias LOF: {anomalias_lof} | Anomalias ISO: {anomalias_iso} | Anomalias (consenso): {total_consenso} ({pct_consenso:.2f}%)\")\n\n# Recomenda√ß√µes:\nprint(\"\\nRecomenda√ß√µes:\")\nprint(\" - Se precisar de uma taxa fixa de anomalias para treino, defina THRESHOLD_ABSOLUTO ap√≥s valida√ß√£o.\")\nprint(\" - Caso contr√°rio, mantenha PERCENTIL_CORTE entre 90 e 98 e ajuste conforme recall/precision desejado.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:12:55.159972Z","iopub.execute_input":"2025-11-11T01:12:55.160193Z","iopub.status.idle":"2025-11-11T01:12:55.200656Z","shell.execute_reply.started":"2025-11-11T01:12:55.160174Z","shell.execute_reply":"2025-11-11T01:12:55.199879Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**PARTE 3: AN√ÅLISE EXPLORAT√ìRIA (ISO/LOF)**","metadata":{}},{"cell_type":"code","source":"# C√âLULA 10: AN√ÅLISE EXPLORAT√ìRIA DO GROUND TRUTH DE CONSENSO\n# ============================================================================\nprint(\"\\n[C√âLULA 10] An√°lise Explorat√≥ria - Ground Truth (Consenso: LOF + ISO)\")\nprint(\"=\" * 80)\n\n# ----------------------------------------------------------------------------\n# [1/6] Separa√ß√£o das amostras normais e an√¥malas com base no consenso\n# ----------------------------------------------------------------------------\nprint(\"\\n  [1/6] Separa√ß√£o das amostras normais e an√¥malas com base no consens\")\nanomalias_df = df_encoded[df_encoded['anomalia_consenso'] == 1]\nnormais_df   = df_encoded[df_encoded['anomalia_consenso'] == 0]\n\ntotal_registros = len(df_encoded)\ntotal_anomalias = len(anomalias_df)\npct_anomalias   = total_anomalias / total_registros * 100\n\nprint(f\"‚úì Total de registros:  {total_registros:,}\")\nprint(f\"‚úì Anomalias (Consenso): {total_anomalias:,} ({pct_anomalias:.2f}%)\")\n\n# ----------------------------------------------------------------------------\n# [2/6] Estat√≠sticas descritivas comparativas\n# ----------------------------------------------------------------------------\nprint(\"\\nüìä [2/6] Estat√≠sticas descritivas - compara√ß√£o entre Normal e An√¥malo\")\n\nfeatures_analise = ['velocidade', 'aceleracao', 'distancia_km', \n                    'mudanca_angulo', 'satellites', 'altitude']\n\nestatisticas = pd.DataFrame({\n    'M√©dia (Normal)': [normais_df[f].mean() for f in features_analise],\n    'M√©dia (An√¥malo)': [anomalias_df[f].mean() for f in features_analise],\n    'Desvio (Normal)': [normais_df[f].std() for f in features_analise],\n    'Desvio (An√¥malo)': [anomalias_df[f].std() for f in features_analise],\n}, index=features_analise)\n\ndisplay(estatisticas.round(3))\n\n# ----------------------------------------------------------------------------\n# [3/6] Gr√°ficos de distribui√ß√£o comparativa\n# ----------------------------------------------------------------------------\nprint(\"\\nüìà [3/6] Distribui√ß√£o das principais vari√°veis (escala logar√≠tmica)\")\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 8))\nfig.suptitle(\"Distribui√ß√µes das Features - Normais vs An√¥malas (Consenso)\", \n             fontsize=16, fontweight='bold', y=1.02)\n\nCOR_NORMAL = '#44AA44'\nCOR_ANOMALIA = '#FF6B6B'\n\nfor i, feature in enumerate(features_analise):\n    ax = axes[i // 3, i % 3]\n    sns.histplot(normais_df[feature], bins=60, color=COR_NORMAL, label='Normal', alpha=0.5, ax=ax)\n    sns.histplot(anomalias_df[feature], bins=60, color=COR_ANOMALIA, label='An√¥malo', alpha=0.5, ax=ax)\n    ax.set_title(feature, fontweight='bold')\n    ax.set_yscale('log')\n    ax.grid(alpha=0.3)\n    if i == 0:\n        ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n# ----------------------------------------------------------------------------\n# [4/6] Rela√ß√£o entre vari√°veis (correla√ß√£o com anomalias)\n# ----------------------------------------------------------------------------\nprint(\"\\nüîç [4/6] Correla√ß√£o das features com a vari√°vel 'anomalia_consenso'\")\n\ncorr = df_encoded[features_analise + ['anomalia_consenso']].corr()['anomalia_consenso'].sort_values(ascending=False)\nplt.figure(figsize=(8, 5))\nsns.barplot(x=corr.values, y=corr.index, palette='coolwarm')\nplt.title(\"Correla√ß√£o das Features com 'anomalia_consenso'\", fontsize=13, fontweight='bold')\nplt.xlabel(\"Correla√ß√£o (Pearson)\")\nplt.ylabel(\"Feature\")\nplt.grid(alpha=0.3)\nplt.show()\n\n# ----------------------------------------------------------------------------\n# [5/6] Densidade espacial (Latitude vs Longitude)\n# ----------------------------------------------------------------------------\n\n\n###### FICA MUITO PESADO PARA RODAR. TIRAR COMENT√ÅRIOS AP√ìS TER 10 AMOSTRAS DOS OUOTPUTS\n\n\n\nprint(\"\\nüó∫Ô∏è [5/6] Distribui√ß√£o espacial das anomalias\")\n\nplt.figure(figsize=(8, 6))\nsns.kdeplot(\n    x=normais_df['Longitude'], y=normais_df['Latitude'],\n    fill=True, cmap='Greens', alpha=0.4, label='Normal'\n)\nsns.scatterplot(\n    x=anomalias_df['Longitude'], y=anomalias_df['Latitude'],\n    color='red', s=10, alpha=0.6, label='Anomalia'\n)\nplt.title(\"Distribui√ß√£o Geogr√°fica - Normais vs An√¥malas (Consenso)\")\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# ----------------------------------------------------------------------------\n# [6/6] Interpreta√ß√£o e diagn√≥stico\n# ----------------------------------------------------------------------------\nprint(\"\\nüß† [6/6] Interpreta√ß√£o dos padr√µes observados\")\nprint(\"-\" * 80)\nprint(\"‚Ä¢ As diferen√ßas m√©dias e as distribui√ß√µes ajudam a entender quais vari√°veis\\n\"\n      \"  mais contribuem para o comportamento an√¥malo (ex: acelera√ß√£o ou varia√ß√£o de √¢ngulo).\\n\")\nprint(\"‚Ä¢ Caso as distribui√ß√µes se sobreponham fortemente, o ensemble ISO+LOF pode estar\\n\"\n      \"  captando ru√≠do, e o LSTM-AE servir√° para filtrar padr√µes temporais mais sutis.\\n\")\nprint(\"‚Ä¢ Se houver separabilidade clara em vari√°veis como 'velocidade' ou 'dist√¢ncia',\\n\"\n      \"  o LSTM deve aprender facilmente a reconstruir apenas trajet√≥rias 'normais'.\")\nprint(\"-\" * 80)\n\nprint(\"\\n‚úÖ An√°lise explorat√≥ria do Ground Truth conclu√≠da ‚Äî dados prontos para o LSTM-AE.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:12:55.201342Z","iopub.execute_input":"2025-11-11T01:12:55.201713Z","iopub.status.idle":"2025-11-11T01:25:17.065131Z","shell.execute_reply.started":"2025-11-11T01:12:55.201694Z","shell.execute_reply":"2025-11-11T01:25:17.064413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# C√âLULA 11: Distribui√ß√£o de Features - Isolation Forest\n# ============================================================================\nprint(\"\\n[C√âLULA 11] An√°lise Explorat√≥ria: O que o ISO detectou?\")\nprint(\"=\" * 80)\n\nanomalias_iso_df = df_encoded[df_encoded['iso_prediction'] == -1]\nnormais_iso_df = df_encoded[df_encoded['iso_prediction'] == 1]\n\nprint(f\"\\nEstat√≠sticas Isolation Forest - Anomalias:\")\nprint(f\"  ‚Ä¢ Velocidade m√©dia: {anomalias_iso_df['velocidade'].mean():.2f} km/h\")\nprint(f\"  ‚Ä¢ Velocidade m√°xima: {anomalias_iso_df['velocidade'].max():.2f} km/h\")\nprint(f\"  ‚Ä¢ Dist√¢ncia m√©dia: {anomalias_iso_df['distancia_km'].mean():.4f} km\")\nprint(f\"  ‚Ä¢ Acelera√ß√£o m√©dia: {anomalias_iso_df['aceleracao'].mean():.2f} m/s¬≤\")\nprint(f\"  ‚Ä¢ Sat√©lites m√©dio: {anomalias_iso_df['satellites'].mean():.1f}\")\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfig.suptitle('Distribui√ß√£o de Features (ISO) - Escala Logar√≠tmica', fontsize=16, fontweight='bold', y=1.00)\n\nCOR_ANOMALIA = '#FF4444'\nCOR_NORMAL = '#44AA44' \n\n# Velocidade\naxes[0, 0].hist(normais_iso_df['velocidade'], bins=50, color=COR_NORMAL, alpha=0.6, label='Normal')\naxes[0, 0].hist(anomalias_iso_df['velocidade'], bins=50, color=COR_ANOMALIA, alpha=0.6, label='Anomalia')\naxes[0, 0].set_xlabel('Velocidade (km/h)', fontweight='bold')\naxes[0, 0].set_ylabel('Frequ√™ncia (Log)', fontweight='bold')\naxes[0, 0].set_title('Velocidade', fontweight='bold')\naxes[0, 0].legend()\naxes[0, 0].grid(alpha=0.3)\naxes[0, 0].set_yscale('log')\n\n# Dist√¢ncia\naxes[0, 1].hist(normais_iso_df['distancia_km'], bins=50, color=COR_NORMAL, alpha=0.6, label='Normal')\naxes[0, 1].hist(anomalias_iso_df['distancia_km'], bins=50, color=COR_ANOMALIA, alpha=0.6, label='Anomalia')\naxes[0, 1].set_xlabel('Dist√¢ncia (km)', fontweight='bold')\naxes[0, 1].set_ylabel('Frequ√™ncia (Log)', fontweight='bold')\naxes[0, 1].set_title('Dist√¢ncia', fontweight='bold')\naxes[0, 1].legend()\naxes[0, 1].grid(alpha=0.3)\naxes[0, 1].set_yscale('log')\n\n# Sat√©lites\naxes[1, 0].hist(normais_iso_df['satellites'], bins=20, color=COR_NORMAL, alpha=0.6, label='Normal')\naxes[1, 0].hist(anomalias_iso_df['satellites'], bins=20, color=COR_ANOMALIA, alpha=0.6, label='Anomalia')\naxes[1, 0].set_xlabel('N√∫mero de Sat√©lites', fontweight='bold')\naxes[1, 0].set_ylabel('Frequ√™ncia (Log)', fontweight='bold')\naxes[1, 0].set_title('Sat√©lites', fontweight='bold')\naxes[1, 0].legend()\naxes[1, 0].grid(alpha=0.3)\naxes[1, 0].set_yscale('log')\n\n# Acelera√ß√£o\naxes[1, 1].hist(normais_iso_df['aceleracao'], bins=50, color=COR_NORMAL, alpha=0.6, label='Normal')\naxes[1, 1].hist(anomalias_iso_df['aceleracao'], bins=50, color=COR_ANOMALIA, alpha=0.6, label='Anomalia')\naxes[1, 1].set_xlabel('Acelera√ß√£o ((km/h)/s)', fontweight='bold')\naxes[1, 1].set_ylabel('Frequ√™ncia (Log)', fontweight='bold')\naxes[1, 1].set_title('Acelera√ß√£o', fontweight='bold')\naxes[1, 1].legend()\naxes[1, 1].grid(alpha=0.3)\naxes[1, 1].set_yscale('log')\n\nplt.tight_layout()\nplt.show()\nprint(\"\\n‚úì Gr√°ficos de distribui√ß√£o Isolation Forest exibidos\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:25:17.066014Z","iopub.execute_input":"2025-11-11T01:25:17.066254Z","iopub.status.idle":"2025-11-11T01:25:19.356343Z","shell.execute_reply.started":"2025-11-11T01:25:17.066235Z","shell.execute_reply":"2025-11-11T01:25:19.355446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# C√âLULA 12: Distribui√ß√£o de Features - LOF\n# ============================================================================\nprint(\"\\n[C√âLULA 12] Distribui√ß√£o de Features - LOF\")\nprint(\"=\" * 80)\n\nanomalias_lof_df = df_encoded[df_encoded['lof_prediction'] == -1]\nnormais_lof_df = df_encoded[df_encoded['lof_prediction'] == 1]\n\nprint(f\"\\nEstat√≠sticas LOF - Anomalias Detectadas:\")\nprint(f\"  ‚Ä¢ Velocidade m√©dia: {anomalias_lof_df['velocidade'].mean():.2f} km/h\")\nprint(f\"  ‚Ä¢ Velocidade m√°xima: {anomalias_lof_df['velocidade'].max():.2f} km/h\")\nprint(f\"  ‚Ä¢ Dist√¢ncia m√©dia: {anomalias_lof_df['distancia_km'].mean():.4f} km\")\nprint(f\"  ‚Ä¢ Acelera√ß√£o m√©dia: {anomalias_lof_df['aceleracao'].mean():.2f} m/s¬≤\")\nprint(f\"  ‚Ä¢ Sat√©lites m√©dio: {anomalias_lof_df['satellites'].mean():.1f}\")\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfig.suptitle('Distribui√ß√£o de Features - LOF', fontsize=16, fontweight='bold', y=1.00)\n\n# Cor para Anomalia LOF (Vermelho)\nCOR_ANOMALIA = '#FF6B6B'\nCOR_NORMAL = '#44AA44'\n\n# Velocidade\naxes[0, 0].hist(normais_lof_df['velocidade'], bins=50, color=COR_NORMAL, alpha=0.6, label='Normal')\naxes[0, 0].hist(anomalias_lof_df['velocidade'], bins=50, color=COR_ANOMALIA, alpha=0.6, label='Anomalia')\naxes[0, 0].set_xlabel('Velocidade (km/h)', fontweight='bold')\naxes[0, 0].set_ylabel('Frequ√™ncia (Log)', fontweight='bold')\naxes[0, 0].set_title('Velocidade', fontweight='bold')\naxes[0, 0].legend()\naxes[0, 0].grid(alpha=0.3)\naxes[0, 0].set_yscale('log') # Melhoria: Usar escala log\n\n# Dist√¢ncia\naxes[0, 1].hist(normais_lof_df['distancia_km'], bins=50, color=COR_NORMAL, alpha=0.6, label='Normal')\naxes[0, 1].hist(anomalias_lof_df['distancia_km'], bins=50, color=COR_ANOMALIA, alpha=0.6, label='Anomalia')\naxes[0, 1].set_xlabel('Dist√¢ncia (km)', fontweight='bold')\naxes[0, 1].set_ylabel('Frequ√™ncia (Log)', fontweight='bold')\naxes[0, 1].set_title('Dist√¢ncia', fontweight='bold')\naxes[0, 1].legend()\naxes[0, 1].grid(alpha=0.3)\naxes[0, 1].set_yscale('log') # Melhoria: Usar escala log\n\n# Sat√©lites\naxes[1, 0].hist(normais_lof_df['satellites'], bins=20, color=COR_NORMAL, alpha=0.6, label='Normal')\naxes[1, 0].hist(anomalias_lof_df['satellites'], bins=20, color=COR_ANOMALIA, alpha=0.6, label='Anomalia')\naxes[1, 0].set_xlabel('N√∫mero de Sat√©lites', fontweight='bold')\naxes[1, 0].set_ylabel('Frequ√™ncia (Log)', fontweight='bold')\naxes[1, 0].set_title('Sat√©lites', fontweight='bold')\naxes[1, 0].legend()\naxes[1, 0].grid(alpha=0.3)\naxes[1, 0].set_yscale('log') # Melhoria: Usar escala log\n\n# Acelera√ß√£o\naxes[1, 1].hist(normais_lof_df['aceleracao'], bins=50, color=COR_NORMAL, alpha=0.6, label='Normal')\naxes[1, 1].hist(anomalias_lof_df['aceleracao'], bins=50, color=COR_ANOMALIA, alpha=0.6, label='Anomalia')\naxes[1, 1].set_xlabel('Acelera√ß√£o (m/s¬≤)', fontweight='bold') # Unidade corrigida\naxes[1, 1].set_ylabel('Frequ√™ncia (Log)', fontweight='bold')\naxes[1, 1].set_title('Acelera√ß√£o', fontweight='bold')\naxes[1, 1].legend()\naxes[1, 1].grid(alpha=0.3)\naxes[1, 1].set_yscale('log') # Melhoria: Usar escala log\n\nplt.tight_layout()\nplt.show()\nprint(\"\\n‚úì Gr√°ficos de distribui√ß√£o LOF exibidos\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:25:19.357283Z","iopub.execute_input":"2025-11-11T01:25:19.358106Z","iopub.status.idle":"2025-11-11T01:25:21.853686Z","shell.execute_reply.started":"2025-11-11T01:25:19.358080Z","shell.execute_reply":"2025-11-11T01:25:21.853018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# C√âLULA 13: Distribui√ß√£o de Features - Isolation Forest\n# ============================================================================\nprint(\"\\n[C√âLULA 13] Distribui√ß√£o de Features - Isolation Forest\")\nprint(\"=\" * 80)\n\nanomalias_iso_df = df_encoded[df_encoded['iso_prediction'] == -1]\nnormais_iso_df = df_encoded[df_encoded['iso_prediction'] == 1]\n\nprint(f\"\\nEstat√≠sticas Isolation Forest - Anomalias:\")\nprint(f\"  ‚Ä¢ Velocidade m√©dia: {anomalias_iso_df['velocidade'].mean():.2f} km/h\")\nprint(f\"  ‚Ä¢ Velocidade m√°xima: {anomalias_iso_df['velocidade'].max():.2f} km/h\")\nprint(f\"  ‚Ä¢ Dist√¢ncia m√©dia: {anomalias_iso_df['distancia_km'].mean():.4f} km\")\nprint(f\"  ‚Ä¢ Acelera√ß√£o m√©dia: {anomalias_iso_df['aceleracao'].mean():.2f} m/s¬≤\")\nprint(f\"  ‚Ä¢ Sat√©lites m√©dio: {anomalias_iso_df['satellites'].mean():.1f}\")\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfig.suptitle('Distribui√ß√£o de Features - Isolation Forest', fontsize=16, fontweight='bold', y=1.00)\n\n# Cor para Anomalia ISO (Azul/Verde)\nCOR_ANOMALIA = '#4ECDC4'\nCOR_NORMAL = '#44AA44'\n\n# Velocidade\naxes[0, 0].hist(normais_iso_df['velocidade'], bins=50, color=COR_NORMAL, alpha=0.6, label='Normal')\naxes[0, 0].hist(anomalias_iso_df['velocidade'], bins=50, color=COR_ANOMALIA, alpha=0.6, label='Anomalia')\naxes[0, 0].set_xlabel('Velocidade (km/h)', fontweight='bold')\naxes[0, 0].set_ylabel('Frequ√™ncia (Log)', fontweight='bold')\naxes[0, 0].set_title('Velocidade', fontweight='bold')\naxes[0, 0].legend()\naxes[0, 0].grid(alpha=0.3)\naxes[0, 0].set_yscale('log') # Melhoria: Usar escala log\n\n# Dist√¢ncia\naxes[0, 1].hist(normais_iso_df['distancia_km'], bins=50, color=COR_NORMAL, alpha=0.6, label='Normal')\naxes[0, 1].hist(anomalias_iso_df['distancia_km'], bins=50, color=COR_ANOMALIA, alpha=0.6, label='Anomalia')\naxes[0, 1].set_xlabel('Dist√¢ncia (km)', fontweight='bold')\naxes[0, 1].set_ylabel('Frequ√™ncia (Log)', fontweight='bold')\naxes[0, 1].set_title('Dist√¢ncia', fontweight='bold')\naxes[0, 1].legend()\naxes[0, 1].grid(alpha=0.3)\naxes[0, 1].set_yscale('log') # Melhoria: Usar escala log\n\n# Sat√©lites\naxes[1, 0].hist(normais_iso_df['satellites'], bins=20, color=COR_NORMAL, alpha=0.6, label='Normal')\naxes[1, 0].hist(anomalias_iso_df['satellites'], bins=20, color=COR_ANOMALIA, alpha=0.6, label='Anomalia')\naxes[1, 0].set_xlabel('N√∫mero de Sat√©lites', fontweight='bold')\naxes[1, 0].set_ylabel('Frequ√™ncia (Log)', fontweight='bold')\naxes[1, 0].set_title('Sat√©lites', fontweight='bold')\naxes[1, 0].legend()\naxes[1, 0].grid(alpha=0.3)\naxes[1, 0].set_yscale('log') # Melhoria: Usar escala log\n\n# Acelera√ß√£o\naxes[1, 1].hist(normais_iso_df['aceleracao'], bins=50, color=COR_NORMAL, alpha=0.6, label='Normal')\naxes[1, 1].hist(anomalias_iso_df['aceleracao'], bins=50, color=COR_ANOMALIA, alpha=0.6, label='Anomalia')\naxes[1, 1].set_xlabel('Acelera√ß√£o (m/s¬≤)', fontweight='bold') # Unidade corrigida\naxes[1, 1].set_ylabel('Frequ√™ncia (Log)', fontweight='bold')\naxes[1, 1].set_title('Acelera√ß√£o', fontweight='bold')\naxes[1, 1].legend()\naxes[1, 1].grid(alpha=0.3)\naxes[1, 1].set_yscale('log') # Melhoria: Usar escala log\n\nplt.tight_layout()\nplt.show()\nprint(\"\\n‚úì Gr√°ficos de distribui√ß√£o Isolation Forest exibidos\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:25:21.854485Z","iopub.execute_input":"2025-11-11T01:25:21.854743Z","iopub.status.idle":"2025-11-11T01:25:24.348221Z","shell.execute_reply.started":"2025-11-11T01:25:21.854720Z","shell.execute_reply":"2025-11-11T01:25:24.347513Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**PARTE 4: PREPARA√á√ÉO E TREINAMENTO (LSTM-AE)**","metadata":{}},{"cell_type":"code","source":"# C√âLULA 14: SELE√á√ÉO DE FEATURES - DOCUMENTADA\n# ============================================================================\nprint(\"\\n[C√âLULA 14] Sele√ß√£o de Features (para LSTM-AE) - Estado + Transi√ß√£o\")\nprint(\"-\" * 80)\n\n# JUSTIFICATIVA (resumida):\n# - LSTM aprende depend√™ncias temporais; n√£o √© necess√°rio alimentar com rolling windows redundantes.\n# - Fornecer estado (posi√ß√£o, velocidade, sinal) + transi√ß√£o (dist√¢ncia, acelera√ß√£o, delta tempo, change of heading)\n# - Evitar lat/lon brutos se preferir usar coordenadas m√©tricas; aqui mantemos lat/lon pois j√° criamos dist√¢ncias.\n\nfeatures_list_lstm = [\n    # Estado geoespacial / movimento\n    'Longitude', 'Latitude', 'altitude', 'angle',\n    'velocidade',\n    # Qualidade GPS\n    'satellites',\n    # Transi√ß√£o / din√¢mica\n    'distancia_km', 'aceleracao', 'delta_tempo_s', 'mudanca_angulo',\n    # Consist√™ncia (novo)\n    'speed_calc_kmh', 'speed_diff_kmh'\n]\n\n# Status (categ√≥ricas) - incluir flags (one-hot) geradas anteriormente\nstatus_cols_lstm = [c for c in df_encoded.columns if c.startswith('status_')]\n\n# FEATURES FINAIS PARA O LSTM\nFEATURES_FINAIS_LSTM = features_list_lstm + status_cols_lstm\n\nprint(f\"‚úì Total de features LSTM: {len(FEATURES_FINAIS_LSTM)}\")\nprint(f\"  - Din√¢micas/estado: {len(features_list_lstm)}\")\nprint(f\"  - Status encoded:   {len(status_cols_lstm)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:25:24.349117Z","iopub.execute_input":"2025-11-11T01:25:24.349399Z","iopub.status.idle":"2025-11-11T01:25:24.355424Z","shell.execute_reply.started":"2025-11-11T01:25:24.349357Z","shell.execute_reply":"2025-11-11T01:25:24.354706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === C√âLULA 15: CRIA√á√ÉO DE SEQU√äNCIAS COM GAP HANDLING (IMPLEMENTA√á√ÉO REVISADA) ===\nprint(\"\\n[C√âLULA 15] Cria√ß√£o de Sequ√™ncias - janelas apenas com o mesmo ve√≠culo e mesmo dia\")\n\nTIME_STEPS = 15           # contexto (ajuste conforme experimentos)\nWAIT_TIME_S = 600         # janela de espera sugerida 600s = 10 min (antes 300s/5min). Teste ambos.\nGAP_THRESHOLD_S = WAIT_TIME_S  # manter consist√™ncia\n\nFEATURE_COLS = FEATURES_FINAIS_LSTM  # lista de features para o LSTM\nLABEL_COL = 'anomalia_consenso'\n\nsequences = []\nseq_labels = []\nseq_masks = []\nmeta = []  # para debug/visualiza√ß√£o (veiculo, start_ts, end_ts)\n\nfor (veh, day), group in df.groupby(['veiculo', 'data_date'], sort=True):\n    group = group.sort_values('timestamp').reset_index(drop=True)\n    # criar indicador de quebra local: se qualquer ruptura_any==1, marcamos e quebramos as sequ√™ncias\n    group['block_id'] = (group['ruptura_any'] == 1).cumsum()\n    # para cada bloco (trecho cont√≠nuo para aquele ve√≠culo naquele dia) criamos janelas deslizantes\n    for bid, block in group.groupby('block_id', sort=True):\n        if len(block) < TIME_STEPS:\n            continue\n        values = block[FEATURE_COLS].values\n        labels_block = block[LABEL_COL].values if LABEL_COL in block.columns else np.zeros(len(block), dtype=int)\n        timestamps = block['timestamp'].values\n\n        # janela deslizante sem atravessar blocos\n        for i in range(0, len(values) - TIME_STEPS + 1):\n            window = values[i:i+TIME_STEPS]\n            window_ts = timestamps[i:i+TIME_STEPS]\n            # checar se tempo entre primeiros e √∫ltimos pontos da janela excede WAIT_TIME (opcional)\n            total_dt = (pd.to_datetime(window_ts[-1]) - pd.to_datetime(window_ts[0])).total_seconds()\n            if total_dt > WAIT_TIME_S * 10:  # sanity check muito grande (evita sequ√™ncias com lacunas enormes)\n                continue\n            # opcional: exigir que o gap m√°ximo dentro da janela seja < GAP_THRESHOLD_S\n            dt_within = np.diff(pd.to_datetime(window_ts).astype('int64') // 10**9)\n            if len(dt_within) > 0 and (dt_within > GAP_THRESHOLD_S).any():\n                continue\n            sequences.append(window.astype(np.float32))\n            # label m√°ximo dentro da janela (se houver qualquer anomalia em qualquer timestep)\n            seq_labels.append(int(labels_block[i:i+TIME_STEPS].max()))\n            seq_masks.append(np.ones(TIME_STEPS, dtype=np.uint8))\n            meta.append({'veiculo': veh, 'date': str(day), 'start': str(window_ts[0]), 'end': str(window_ts[-1])})\n\n# converter para arrays\nimport numpy as np\nX = np.array(sequences)\ny = np.array(seq_labels)\nmasks = np.array(seq_masks)\n\nprint(f\"Sequ√™ncias criadas: {len(X)}, shape X: {X.shape} y: {y.shape}\")\nprint(\"Sugest√£o: experimente WAIT_TIME_S = 300 (5min) e = 600 (10min). Use valida√ß√£o para escolher melhor.\")\n\n# salvar para o pipeline\nnp.save('/mnt/data/X_sequences.npy', X)\nnp.save('/mnt/data/y_sequences.npy', y)\nnp.save('/mnt/data/masks_sequences.npy', masks)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:25:24.356240Z","iopub.execute_input":"2025-11-11T01:25:24.356502Z","iopub.status.idle":"2025-11-11T01:27:18.033031Z","shell.execute_reply.started":"2025-11-11T01:25:24.356482Z","shell.execute_reply":"2025-11-11T01:27:18.032354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# C√âLULA 16: SEPARA√á√ÉO TEMPORAL E PREPARA√á√ÉO DO PIPELINE (NORMALIZA√á√ÉO SEMI-SUPERVISIONADA)\n# ============================================================================\nprint(\"\\n[C√âLULA 16] Separa√ß√£o temporal (treino/valida√ß√£o/teste) e cria√ß√£o de datasets TF\")\nprint(\"-\" * 80)\n\n# Separar sequ√™ncias normais e an√¥malas\nX_normal = X[y == 0]\ny_normal = y[y == 0]\nX_anomalo = X[y == 1]\ny_anomalo = y[y == 1]\n\nprint(f\"Sequ√™ncias normais: {len(X_normal):,} | an√¥malas: {len(X_anomalo):,}\")\n\n# Ordena√ß√£o temporal ‚Äî assumimos que as sequ√™ncias j√° est√£o em ordem temporal por constru√ß√£o\n# Treino: primeiros 80% das sequ√™ncias normais\nsplit_idx = int(0.8 * len(X_normal))\nX_train_full = X_normal[:split_idx]\nX_val = X_normal[split_idx:]   # √∫ltimos 20% dos normais usados para valida√ß√£o\ny_val = np.zeros(len(X_val), dtype=int)  # labels de valida√ß√£o s√£o todos 0 (normais)\n\nprint(f\"Treino: {len(X_train_full):,} sequ√™ncias (normais)\")\nprint(f\"Valida√ß√£o (normais): {len(X_val):,} sequ√™ncias\")\n\n# Teste: combina√ß√£o de X_val (normais) + X_anomalo (anomalias do consenso)\nX_test = np.concatenate([X_val, X_anomalo], axis=0)\ny_test = np.concatenate([y_val, y_anomalo], axis=0)\n\n# --- Normaliza√ß√£o: fit apenas nos normais do treino (semi-supervisionado)\n# Fazer MinMax por feature (0..1) usando flatten sobre todas as timesteps\nn_timesteps = X_train_full.shape[1]\nn_features = X_train_full.shape[2]\n\n# Reformatar para (samples * timesteps, features) para scaler, depois transformar de volta\nX_train_2d = X_train_full.reshape(-1, n_features)  # rows = samples * timesteps\nscaler_lstm = MinMaxScaler()\nscaler_lstm.fit(X_train_2d)\n\n# Fun√ß√£o utilit√°ria para transformar conjuntos\ndef scale_sequences(X_input, scaler, n_timesteps, n_features):\n    \"\"\"\n    Recebe X_input shape=(N, time_steps, n_features)\n    Retorna X_scaled com mesma shape, usando o scaler ajustado.\n    \"\"\"\n    s = X_input.reshape(-1, n_features)\n    s_scaled = scaler.transform(s)\n    return s_scaled.reshape(-1, n_timesteps, n_features)\n\n# Aplicar scaler\nX_train = scale_sequences(X_train_full, scaler_lstm, n_timesteps, n_features)\nX_val_scaled = scale_sequences(X_val, scaler_lstm, n_timesteps, n_features)\nX_test_scaled = scale_sequences(X_test, scaler_lstm, n_timesteps, n_features)\n\nprint(\"‚úì Normaliza√ß√£o semi-supervisionada aplicada (fit apenas em normais de treino)\")\n\n# Amostragem para limitar mem√≥ria/GPU se necess√°rio (opcional)\nN_AMOSTRAS = 100000\nif len(X_train) > N_AMOSTRAS:\n    idxs = np.random.choice(len(X_train), N_AMOSTRAS, replace=False)\n    X_train = X_train[idxs]\n\n# --- Criar pipelines TF (sem shuffle temporal internamente; shuffle entre batches OK)\nBATCH_SIZE = 64\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, X_train))\n# N√£o embaralhar sequ√™ncias dentro da sequ√™ncia; shuffle das janelas √© aceit√°vel para generaliza√ß√£o\ntrain_dataset = train_dataset.cache().shuffle(buffer_size=10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((X_val_scaled, X_val_scaled)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\nprint(f\"‚úì Datasets criados: Treino={X_train.shape}, Val={X_val_scaled.shape}, Test={X_test_scaled.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:27:18.033742Z","iopub.execute_input":"2025-11-11T01:27:18.033968Z","iopub.status.idle":"2025-11-11T01:27:22.168289Z","shell.execute_reply.started":"2025-11-11T01:27:18.033943Z","shell.execute_reply":"2025-11-11T01:27:22.167627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# C√âLULA 17: DEFINI√á√ÉO DO MODELO LSTM-AE\n# ============================================================================\nprint(\"\\n[C√âLULA 17] Defini√ß√£o do Modelo LSTM-Autoencoder (com regulariza√ß√£o)\")\nprint(\"-\" * 80)\n\ntimesteps = TIME_STEPS\nn_features = n_features  \n\n# Hiperpar√¢metros (ajust√°veis)\nENCODER_UNITS = 64\nDECODER_UNITS = 64\nDROPOUT_RATE = 0.2\n\n# Arquitetura:\n# - Encoder LSTM (compress√£o temporal)\n# - Bottleneck (vetor latente)\n# - RepeatVector para sequ√™ncia\n# - Decoder LSTM (reconstru√ß√£o)\n# - TimeDistributed Dense com ativa√ß√£o linear (reconstru√≠mos valores normalizados)\ninput_layer = Input(shape=(timesteps, n_features), name='input_seq')\n\n# Encoder\nx = LSTM(ENCODER_UNITS, return_sequences=False, name='encoder_lstm')(input_layer)\nx = LayerNormalization(name='encoder_ln')(x)\nx = Dropout(DROPOUT_RATE, name='encoder_dropout')(x)\n\n# Bottleneck -> repetir\nx = RepeatVector(timesteps, name='repeat_vector')(x)\n\n# Decoder\nx = LSTM(DECODER_UNITS, return_sequences=True, name='decoder_lstm')(x)\nx = LayerNormalization(name='decoder_ln')(x)\nx = Dropout(DROPOUT_RATE, name='decoder_dropout')(x)\n\n# Output (linear porque os dados j√° est√£o escalados para [0,1] via MinMaxScaler)\noutput_layer = TimeDistributed(Dense(n_features, activation='linear'), name='time_distributed_out')(x)\n\nautoencoder = Model(inputs=input_layer, outputs=output_layer, name='lstm_autoencoder')\n\n# Compilar com MAE\nautoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mae')\n\nautoencoder.summary()\nprint(\"\\n‚úì Modelo LSTM-AE pronto. Use EarlyStopping e restore_best_weights no treino.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:27:22.169015Z","iopub.execute_input":"2025-11-11T01:27:22.169190Z","iopub.status.idle":"2025-11-11T01:27:23.481140Z","shell.execute_reply.started":"2025-11-11T01:27:22.169169Z","shell.execute_reply":"2025-11-11T01:27:23.480563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# C√âLULA 18: TREINAMENTO DO MODELO LSTM-AE COM CALLBACKS E LOGGING\n# ============================================================================\nprint(\"\\n[C√âLULA 18] Treinamento do LSTM-Autoencoder\")\nprint(\"=\" * 80)\n\n# ----------------------------------------------------------------------------\n# [1/5] Configura√ß√µes de logging e diret√≥rios\n# ----------------------------------------------------------------------------\nDATA_HOJE = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nDIR_LOGS = f\"./logs/LSTM_AE_{DATA_HOJE}\"\nDIR_MODELOS = f\"./modelos\"\nos.makedirs(DIR_LOGS, exist_ok=True)\nos.makedirs(DIR_MODELOS, exist_ok=True)\n\n# ----------------------------------------------------------------------------\n# [2/5] Callbacks\n# ----------------------------------------------------------------------------\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=10,\n    restore_best_weights=True,\n    verbose=1\n)\n\ncheckpoint = ModelCheckpoint(\n    filepath=os.path.join(DIR_MODELOS, f\"lstm_ae_best_{DATA_HOJE}.h5\"),\n    monitor='val_loss',\n    save_best_only=True,\n    verbose=1\n)\n\ntensorboard_cb = TensorBoard(\n    log_dir=DIR_LOGS,\n    histogram_freq=1,\n    write_graph=True,\n    write_images=False\n)\n\n# ----------------------------------------------------------------------------\n# [3/5] Treinamento\n# ----------------------------------------------------------------------------\nEPOCHS = 60\nBATCH_SIZE = 64\n\nprint(f\"Treinando por at√© {EPOCHS} √©pocas com batch_size={BATCH_SIZE}\")\nprint(f\"Logs: {DIR_LOGS}\")\n\nhistory = autoencoder.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=EPOCHS,\n    callbacks=[early_stopping, checkpoint, tensorboard_cb],\n    verbose=1\n)\n\n# ----------------------------------------------------------------------------\n# [4/5] Gr√°ficos de converg√™ncia\n# ----------------------------------------------------------------------------\nplt.figure(figsize=(8, 5))\nplt.plot(history.history['loss'], label='Treino', linewidth=2)\nplt.plot(history.history['val_loss'], label='Valida√ß√£o', linewidth=2, linestyle='--')\nplt.xlabel('√âpocas')\nplt.ylabel('MAE (Loss)')\nplt.title('Curva de Treinamento - LSTM Autoencoder')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# ----------------------------------------------------------------------------\n# [5/5] Salvamento final\n# ----------------------------------------------------------------------------\nmodelo_final_path = os.path.join(DIR_MODELOS, f\"lstm_ae_final_{DATA_HOJE}.h5\")\nautoencoder.save(modelo_final_path)\nprint(f\"\\n‚úÖ Modelo salvo: {modelo_final_path}\")\nprint(f\"üí° TensorBoard ‚Üí execute: tensorboard --logdir {DIR_LOGS}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:27:23.481780Z","iopub.execute_input":"2025-11-11T01:27:23.481957Z","iopub.status.idle":"2025-11-11T01:39:55.420004Z","shell.execute_reply.started":"2025-11-11T01:27:23.481942Z","shell.execute_reply":"2025-11-11T01:39:55.419400Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**PARTE 5: AVALIA√á√ÉO FINAL DO ENSEMBLE**","metadata":{}},{"cell_type":"code","source":"# C√âLULA 19: AVALIA√á√ÉO E THRESHOLD (DETEC√á√ÉO DE ANOMALIAS)\n# ============================================================================\nprint(\"\\n[C√âLULA 19] Avalia√ß√£o do LSTM-AE - Threshold e M√©tricas de Anomalia\")\nprint(\"=\" * 80)\n\n# ----------------------------------------------------------------------------\n# [1/6] Reconstru√ß√£o no conjunto de teste\n# ----------------------------------------------------------------------------\nprint(\"Reconstruindo sequ√™ncias de teste...\")\n\nX_pred = autoencoder.predict(X_test_scaled, verbose=0)\nreconstruction_error = np.mean(np.abs(X_pred - X_test_scaled), axis=(1,2))\n\n# ----------------------------------------------------------------------------\n# [2/6] Distribui√ß√£o do erro\n# ----------------------------------------------------------------------------\nplt.figure(figsize=(8,5))\nsns.histplot(reconstruction_error, bins=80, kde=True, color='purple', alpha=0.6)\nplt.title('Distribui√ß√£o do Erro de Reconstru√ß√£o (Teste)')\nplt.xlabel('MAE por sequ√™ncia')\nplt.ylabel('Frequ√™ncia')\nplt.grid(alpha=0.3)\nplt.show()\n\n# ----------------------------------------------------------------------------\n# [3/6] Escolha do threshold\n# ----------------------------------------------------------------------------\n# Estrat√©gias:\n#   a) Percentil emp√≠rico (90, 95, 97, 99)\n#   b) Maximiza√ß√£o de F1 via curva Precision-Recall\n\n# a) Percentil simples\nPERCENTIL_CORTE = 97\nthreshold_perc = np.percentile(reconstruction_error, PERCENTIL_CORTE)\nprint(f\"Threshold (percentil {PERCENTIL_CORTE}%): {threshold_perc:.5f}\")\n\n# b) Curva Precision-Recall (baseada em labels conhecidas)\nprecisions, recalls, thresholds_pr = precision_recall_curve(y_test, reconstruction_error)\nf1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\nidx_best = np.argmax(f1_scores)\nthreshold_best = thresholds_pr[idx_best]\nprint(f\"Threshold (melhor F1): {threshold_best:.5f}\")\n\n# Escolher abordagem preferida\nTHRESHOLD = threshold_perc  # ou threshold_best\n\n# ----------------------------------------------------------------------------\n# [4/6] Predi√ß√£o final e m√©tricas\n# ----------------------------------------------------------------------------\ny_pred = (reconstruction_error > THRESHOLD).astype(int)\n\nauc_score = roc_auc_score(y_test, reconstruction_error)\nprint(f\"\\nROC-AUC: {auc_score:.4f}\")\n\ncm = confusion_matrix(y_test, y_pred)\ntn, fp, fn, tp = cm.ravel()\nprec = tp / (tp + fp + 1e-10)\nrec = tp / (tp + fn + 1e-10)\nf1 = 2 * (prec * rec) / (prec + rec + 1e-10)\n\nprint(\"\\nüìä MATRIZ DE CONFUS√ÉO\")\nprint(pd.DataFrame(cm, index=['Real: Normal', 'Real: An√¥malo'], columns=['Pred: Normal', 'Pred: An√¥malo']))\nprint(f\"\\nPrecis√£o: {prec:.3f} | Recall: {rec:.3f} | F1-score: {f1:.3f}\")\n\n# ----------------------------------------------------------------------------\n# [5/6] Visualiza√ß√£o do threshold e erros\n# ----------------------------------------------------------------------------\nplt.figure(figsize=(10,5))\nsns.scatterplot(x=np.arange(len(reconstruction_error)), y=reconstruction_error,\n                hue=y_test, palette={0: 'green', 1: 'red'}, alpha=0.6)\nplt.axhline(THRESHOLD, color='black', linestyle='--', label=f'Threshold = {THRESHOLD:.5f}')\nplt.title('Erro de Reconstru√ß√£o vs. Ground Truth (Teste)')\nplt.xlabel('√çndice da sequ√™ncia')\nplt.ylabel('Erro MAE')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# ----------------------------------------------------------------------------\n# [6/6] Relat√≥rio final\n# ----------------------------------------------------------------------------\nprint(\"\\nüß† RELAT√ìRIO FINAL\")\nprint(\"-\" * 100)\nprint(f\"Threshold usado: {THRESHOLD:.5f}\")\nprint(f\"ROC-AUC: {auc_score:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall: {rec:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\nprint(f\"TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\")\nprint(\"-\" * 100)\n\nprint(\"\\n‚úÖ Avalia√ß√£o conclu√≠da. Modelo pronto para an√°lise de reconstru√ß√£o detalhada e tuning fino.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:39:55.420842Z","iopub.execute_input":"2025-11-11T01:39:55.421077Z","iopub.status.idle":"2025-11-11T01:40:02.715788Z","shell.execute_reply.started":"2025-11-11T01:39:55.421050Z","shell.execute_reply":"2025-11-11T01:40:02.714925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# C√âLULA 20: AVALIA√á√ÉO FINAL DO ENSEMBLE (LSTM-AE)\n# ============================================================================\nprint(\"\\n[C√âLULA 20] Avalia√ß√£o Final (LSTM-AE vs. Ground Truth de Consenso)\")\nprint(\"=\" * 80)\n\n# Garantir que X_test est√° em formato compat√≠vel com Keras\nX_test = np.asarray(X_test).astype(\"float32\")\n\n# 1Ô∏è‚É£ Previs√£o no conjunto de teste\nX_test_pred = autoencoder.predict(X_test, batch_size=BATCH_SIZE)\n\n# 2Ô∏è‚É£ Calcular o erro de reconstru√ß√£o (MAE) para cada sequ√™ncia\ntest_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=(1, 2))\n\n# 3Ô∏è‚É£ Definir o limiar (caso n√£o esteja definido antes)\nif 'threshold' not in locals():\n    threshold = np.mean(test_mae_loss) + 3 * np.std(test_mae_loss)\n    print(f\"Limiar definido automaticamente: {threshold:.4f}\")\n\n# 4Ô∏è‚É£ Classificar sequ√™ncias como normais (0) ou an√¥malas (1)\ny_pred_lstm = (test_mae_loss > threshold).astype(int)\n\n# 5Ô∏è‚É£ Separar erros por classe real (para visualiza√ß√£o)\nerros_normais_teste = test_mae_loss[y_test == 0]\nerros_anomalos_teste = test_mae_loss[y_test == 1]\n\nprint(f\"Total de Sequ√™ncias de Teste: {len(y_test)}\")\nprint(f\"  ‚Ä¢ Anomalias Reais (Consenso GT): {(y_test == 1).sum():,}\")\nprint(f\"  ‚Ä¢ Anomalias Detectadas (LSTM):   {(y_pred_lstm == 1).sum():,}\")\n\n# 6Ô∏è‚É£ Plotar distribui√ß√£o dos erros de reconstru√ß√£o\nplt.figure(figsize=(10, 6))\nsns.histplot(erros_normais_teste, bins=50, kde=True, color='blue', label='Sequ√™ncias Normais (Consenso)')\nsns.histplot(erros_anomalos_teste, bins=50, kde=True, color='red', label='Sequ√™ncias An√¥malas (Consenso)')\nplt.axvline(threshold, color='black', linestyle='--', linewidth=2, label=f'Limiar ({threshold:.4f})')\nplt.title('Distribui√ß√£o de Erros no Conjunto de Teste')\nplt.xlabel('Erro de Reconstru√ß√£o (MAE)')\nplt.ylabel('Frequ√™ncia')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:40:02.716572Z","iopub.execute_input":"2025-11-11T01:40:02.716804Z","iopub.status.idle":"2025-11-11T01:40:06.234872Z","shell.execute_reply.started":"2025-11-11T01:40:02.716786Z","shell.execute_reply":"2025-11-11T01:40:06.234060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# C√âLULA 21: TIMELINE E MAPA DE ANOMALIAS (LSTM-AE + GPS)\n# ============================================================================\nprint(\"\\n[C√âLULA 21] Timeline e Mapa de Anomalias (por ve√≠culo)\")\nprint(\"=\" * 80)\n\nimport matplotlib.dates as mdates\nimport folium\nfrom folium.plugins import HeatMap\n\n# ----------------------------------------------------------------------------\n# [1/6] Reconstruir DataFrame de avalia√ß√£o\n# ----------------------------------------------------------------------------\nprint(\"Reconstruindo DataFrame de avalia√ß√£o...\")\n\ndf_resultados = pd.DataFrame({\n    'veiculo': df_model_data.iloc[-len(y_test):]['veiculo'].values,\n    'data': df_model_data.iloc[-len(y_test):]['data'].values,\n    'Latitude': df_model_data.iloc[-len(y_test):]['Latitude'].values,\n    'Longitude': df_model_data.iloc[-len(y_test):]['Longitude'].values,\n    'erro_reconstrucao': reconstruction_error,\n    'y_real': y_test,\n    'y_pred': y_pred\n})\n\n# ----------------------------------------------------------------------------\n# [2/6] Timeline do erro de reconstru√ß√£o\n# ----------------------------------------------------------------------------\nprint(\"Plotando timeline de erros...\")\n\n# Selecionar ve√≠culo com maior quantidade de anomalias para exemplo\nveic_mais_anomalias = (\n    df_resultados[df_resultados['y_pred'] == 1]['veiculo']\n    .value_counts()\n    .idxmax()\n)\ndf_plot = df_resultados[df_resultados['veiculo'] == veic_mais_anomalias].sort_values('data')\n\nplt.figure(figsize=(12,5))\nplt.plot(df_plot['data'], df_plot['erro_reconstrucao'], label='Erro de reconstru√ß√£o', color='purple', linewidth=1.5)\nplt.scatter(\n    df_plot.loc[df_plot['y_pred'] == 1, 'data'],\n    df_plot.loc[df_plot['y_pred'] == 1, 'erro_reconstrucao'],\n    color='red', s=30, label='Anomalia detectada'\n)\nplt.axhline(THRESHOLD, color='black', linestyle='--', label=f'Threshold = {THRESHOLD:.5f}')\nplt.title(f'üìà Timeline de Erros - Ve√≠culo {veic_mais_anomalias}', fontweight='bold')\nplt.xlabel('Data/Hora')\nplt.ylabel('Erro de Reconstru√ß√£o (MAE)')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m %H:%M'))\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# ----------------------------------------------------------------------------\n# [3/6] Resumo estat√≠stico\n# ----------------------------------------------------------------------------\nanomalias_veic = df_plot[df_plot['y_pred'] == 1]\npct_anomalias = len(anomalias_veic) / len(df_plot) * 100\nprint(f\"\\nResumo Ve√≠culo {veic_mais_anomalias}:\")\nprint(f\"  ‚Ä¢ Registros totais: {len(df_plot):,}\")\nprint(f\"  ‚Ä¢ Anomalias detectadas: {len(anomalias_veic):,} ({pct_anomalias:.2f}%)\")\nprint(f\"  ‚Ä¢ M√©dia erro normal: {df_plot[df_plot['y_pred']==0]['erro_reconstrucao'].mean():.4f}\")\nprint(f\"  ‚Ä¢ M√©dia erro an√¥malo: {anomalias_veic['erro_reconstrucao'].mean():.4f}\")\n\n# ----------------------------------------------------------------------------\n# [4/6] Mapa interativo de anomalias (Folium)\n# ----------------------------------------------------------------------------\nprint(\"Gerando mapa interativo...\")\n\n# Ponto inicial do mapa (centro das anomalias)\nif len(anomalias_veic) > 0:\n    centro_lat = anomalias_veic['Latitude'].mean()\n    centro_lon = anomalias_veic['Longitude'].mean()\nelse:\n    centro_lat = df_plot['Latitude'].mean()\n    centro_lon = df_plot['Longitude'].mean()\n\nmapa = folium.Map(location=[centro_lat, centro_lon], zoom_start=12, tiles='CartoDB positron')\n\n# Adicionar pontos an√¥malos\nfor _, row in anomalias_veic.iterrows():\n    folium.CircleMarker(\n        location=[row['Latitude'], row['Longitude']],\n        radius=5,\n        color='red',\n        fill=True,\n        fill_opacity=0.7,\n        popup=f\"{row['data']}<br>Erro: {row['erro_reconstrucao']:.4f}\"\n    ).add_to(mapa)\n\n# Adicionar heatmap\nHeatMap(data=anomalias_veic[['Latitude', 'Longitude']], radius=15, blur=10, min_opacity=0.4).add_to(mapa)\n\n# Salvar o mapa\nmapa_path = f\"mapa_anomalias_{veic_mais_anomalias}.html\"\nmapa.save(mapa_path)\nprint(f\"\\n‚úÖ Mapa salvo em: {mapa_path}\")\nprint(\"üí° Voc√™ pode visualizar o mapa clicando no arquivo HTML gerado no painel do Kaggle.\")\n\n# ----------------------------------------------------------------------------\n# [5/6] Heatmap global de anomalias (todos os ve√≠culos)\n# ----------------------------------------------------------------------------\nprint(\"Gerando heatmap global de anomalias...\")\n\ndf_anom_global = df_resultados[df_resultados['y_pred'] == 1].copy()\nif len(df_anom_global) > 0:\n    mapa_global = folium.Map(location=[df_anom_global['Latitude'].mean(),\n                                       df_anom_global['Longitude'].mean()],\n                             zoom_start=6, tiles='CartoDB positron')\n    HeatMap(data=df_anom_global[['Latitude', 'Longitude']], radius=8, blur=6, min_opacity=0.3).add_to(mapa_global)\n    mapa_global.save(\"heatmap_global_anomalias.html\")\n    print(\"‚úÖ Heatmap global salvo: heatmap_global_anomalias.html\")\nelse:\n    print(\"‚ö†Ô∏è Nenhuma anomalia detectada globalmente.\")\n\n# ----------------------------------------------------------------------------\n# [6/6] Conclus√£o\n# ----------------------------------------------------------------------------\nprint(\"\\nüó∫Ô∏è  Visualiza√ß√£o conclu√≠da.\")\nprint(\" - Timeline por ve√≠culo: ‚úîÔ∏è\")\nprint(\" - Mapa interativo Folium: ‚úîÔ∏è\")\nprint(\" - Heatmap global: ‚úîÔ∏è\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:40:06.235736Z","iopub.execute_input":"2025-11-11T01:40:06.235995Z","iopub.status.idle":"2025-11-11T01:40:06.881132Z","shell.execute_reply.started":"2025-11-11T01:40:06.235974Z","shell.execute_reply":"2025-11-11T01:40:06.880546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# C√âLULA 22: EXPORTA√á√ÉO E CONSOLIDA√á√ÉO DOS RESULTADOS\n# ============================================================================\nprint(\"\\n[C√âLULA 22] Exporta√ß√£o e Consolida√ß√£o Final dos Resultados\")\nprint(\"=\" * 80)\n\n# Esta c√©lula salva os resultados combinando LOF, ISO e LSTM-AE\n# de forma que possam ser analisados posteriormente em Power BI,\n# Superset ou notebooks auxiliares.\n\n# ---------------------------------------------------------------------------\n# 1Ô∏è‚É£ Reconstruir o dataframe principal com todos os resultados relevantes\n# ---------------------------------------------------------------------------\n\nprint(\"üîÑ Consolidando resultados no DataFrame principal...\")\n\n# Garantir que os vetores de predi√ß√£o existam\nif 'reconstruction_error' not in locals():\n    print(\"‚ö†Ô∏è  Erro: vari√°vel 'reconstruction_error' n√£o encontrada. Execute a c√©lula 22 antes desta.\")\nelse:\n    # Converter erro e predi√ß√µes em DataFrame\n    resultados_lstm = pd.DataFrame({\n        'data_seq': timestamps[-len(y_test):],          # √öltimos timestamps do conjunto de teste\n        'y_real_consenso': y_test,                      # Ground Truth (LOF+ISO)\n        'y_pred_lstm': y_pred,                          # Predi√ß√£o do LSTM-AE\n        'reconstruction_error': reconstruction_error,   # Erro de reconstru√ß√£o por sequ√™ncia\n    })\n\n    # Adicionar flag textual para facilitar visualiza√ß√£o em BI\n    resultados_lstm['status_predito'] = np.where(resultados_lstm['y_pred_lstm'] == 1, \n                                                 'Anomalia', 'Normal')\n\n    print(f\"‚úì Linhas consolidadas: {len(resultados_lstm):,}\")\n    print(f\"‚úì Colunas: {list(resultados_lstm.columns)}\")\n\n    # -----------------------------------------------------------------------\n    # 2Ô∏è‚É£ Exportar DataFrames intermedi√°rios para an√°lise posterior\n    # -----------------------------------------------------------------------\n    print(\"\\nüíæ Exportando DataFrames para arquivos CSV...\")\n\n    # Dataset LOF/ISO completo (inclui ground truth e scores)\n    caminho_lof_iso = '/kaggle/working/df_lof_iso.csv'\n    df_encoded.to_csv(caminho_lof_iso, index=False)\n\n    # Dataset de resultados do LSTM-AE (temporal)\n    caminho_lstm = '/kaggle/working/resultados_lstm.csv'\n    resultados_lstm.to_csv(caminho_lstm, index=False)\n\n    print(f\"‚úì LOF/ISO exportado para: {caminho_lof_iso}\")\n    print(f\"‚úì LSTM-AE exportado para: {caminho_lstm}\")\n\n    # -----------------------------------------------------------------------\n    # 3Ô∏è‚É£ Estat√≠sticas gerais finais\n    # -----------------------------------------------------------------------\n    print(\"\\nüìä Estat√≠sticas Finais (para relat√≥rio)\")\n\n    total = len(resultados_lstm)\n    anom_real = resultados_lstm['y_real_consenso'].sum()\n    anom_pred = resultados_lstm['y_pred_lstm'].sum()\n    precisao = (resultados_lstm['y_real_consenso'] == resultados_lstm['y_pred_lstm']).mean() * 100\n\n    print(f\"Total de sequ√™ncias avaliadas: {total:,}\")\n    print(f\"Anomalias reais (GT): {anom_real:,} ({anom_real/total*100:.2f}%)\")\n    print(f\"Anomalias detectadas (LSTM): {anom_pred:,} ({anom_pred/total*100:.2f}%)\")\n    print(f\"Acur√°cia global (consenso vs LSTM): {precisao:.2f}%\")\n\n    # -----------------------------------------------------------------------\n    # 4Ô∏è‚É£ Exportar vers√£o compactada (parquet) ‚Äî opcional, para grande volume\n    # -----------------------------------------------------------------------\n    print(\"\\nüì¶ Gerando vers√£o compactada (Parquet) para an√°lise r√°pida...\")\n    caminho_parquet = '/kaggle/working/resultados_lstm.parquet'\n    resultados_lstm.to_parquet(caminho_parquet, index=False)\n    print(f\"‚úì Arquivo compactado salvo em: {caminho_parquet}\")\n\n    # -----------------------------------------------------------------------\n    # 5Ô∏è‚É£ Log final de encerramento\n    # -----------------------------------------------------------------------\n    print(\"\\n‚úÖ Exporta√ß√£o conclu√≠da com sucesso.\")\n    print(\"   ‚Üí Voc√™ pode baixar os arquivos no painel 'Output Files' do Kaggle.\")\n    print(\"   ‚Üí Recomendado: importar o CSV ou Parquet em Power BI para an√°lises temporais e espaciais.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T01:40:06.881893Z","iopub.execute_input":"2025-11-11T01:40:06.882366Z","iopub.status.idle":"2025-11-11T01:40:39.769559Z","shell.execute_reply.started":"2025-11-11T01:40:06.882347Z","shell.execute_reply":"2025-11-11T01:40:39.768817Z"}},"outputs":[],"execution_count":null}]}